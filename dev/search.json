[
  {
    "objectID": "reference/GSEA.multitext2excel.html",
    "href": "reference/GSEA.multitext2excel.html",
    "title": "GSEA.multitext2excel",
    "section": "",
    "text": "GSEA.multitext2excel\nGSEA.multitext2excel\nReads a list of files to import as separate tabs in Excel\nCreated on Mon Aug 6 14:59:13 2018\nSusan Huse NIAID Center for Biological Research Frederick National Laboratory for Cancer Research Leidos Biomedical\nv 1.0 - initial code version. v 1.1 - updated to include first splitter markowitzte@nih.gov",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "GSEA.multitext2excel"
    ]
  },
  {
    "objectID": "reference/GSEA.deg2gs.html",
    "href": "reference/GSEA.deg2gs.html",
    "title": "GSEA.deg2gs",
    "section": "",
    "text": "GSEA.deg2gs\nGSEA.deg2gs\nReads a rnaseq pipeliner *_DEG_all_genes.txt file and outputs a prioritized list of Ensembl gene IDs for ToppFun\nAuthor: Susan Huse\nNIAID Center for Biological Research\nFrederick National Laboratory for Cancer Research\nLeidos Biomedical\n\nv 1.0 - initial code version.\nv 1.1 - updated for new column headers in pipeliner limma_DEG_all_genes.txt\nv 1.2 - top2Excel format is now csv rather than tab-delimited",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "GSEA.deg2gs"
    ]
  },
  {
    "objectID": "reference/pipeline.nextflow.html",
    "href": "reference/pipeline.nextflow.html",
    "title": "pipeline.nextflow",
    "section": "",
    "text": "pipeline.nextflow\nRun Nextflow workflows in local and HPC environments.\n\nrun(nextfile_path=None, nextflow_args=None, mode=“local”, pipeline_name=None, debug=False, hpc_options={}) Run a Nextflow workflow.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nrun\nRun a Nextflow workflow\n\n\n\n\n\npipeline.nextflow.run(\n    nextfile_path\n    nextflow_args=[]\n    mode='local'\n    pipeline_name=None\n    debug=False\n)\nRun a Nextflow workflow\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnextfile_path\nstr\nPath to the Nextflow file.\nrequired\n\n\nnextflow_args\nlist\nAdditional Nextflow arguments. Defaults to an empty list.\n[]\n\n\nmode\nstr\nExecution mode. Defaults to “local”.\n'local'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf mode is ‘slurm’ but no HPC environment was detected.",
    "crumbs": [
      "Reference",
      "Modules",
      "pipeline.nextflow"
    ]
  },
  {
    "objectID": "reference/pipeline.nextflow.html#functions",
    "href": "reference/pipeline.nextflow.html#functions",
    "title": "pipeline.nextflow",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nrun\nRun a Nextflow workflow\n\n\n\n\n\npipeline.nextflow.run(\n    nextfile_path\n    nextflow_args=[]\n    mode='local'\n    pipeline_name=None\n    debug=False\n)\nRun a Nextflow workflow\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnextfile_path\nstr\nPath to the Nextflow file.\nrequired\n\n\nnextflow_args\nlist\nAdditional Nextflow arguments. Defaults to an empty list.\n[]\n\n\nmode\nstr\nExecution mode. Defaults to “local”.\n'local'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf mode is ‘slurm’ but no HPC environment was detected.",
    "crumbs": [
      "Reference",
      "Modules",
      "pipeline.nextflow"
    ]
  },
  {
    "objectID": "reference/pipeline.cache.html",
    "href": "reference/pipeline.cache.html",
    "title": "pipeline.cache",
    "section": "",
    "text": "pipeline.cache\nFunctions for singularity cache management\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_cache\nCheck if provided SINGULARITY_CACHE is valid. Singularity caches cannot be\n\n\nget_singularity_cachedir\nReturns the singularity cache directory.\n\n\nimage_cache\nAdds Docker Image URIs, or SIF paths to config if singularity cache option is provided.\n\n\n\n\n\npipeline.cache.check_cache(parser, cache, *args, **kwargs)\nCheck if provided SINGULARITY_CACHE is valid. Singularity caches cannot be shared across users (and must be owned by the user). Singularity strictly enforces 0700 user permission on on the cache directory and will return a non-zero exitcode. @param parser &lt;argparse.ArgumentParser() object&gt;: Argparse parser object @param cache : Singularity cache directory @return cache : If singularity cache dir is valid\n\n\n\npipeline.cache.get_singularity_cachedir(output_dir=None, cache_dir=None)\nReturns the singularity cache directory. If no user-provided cache directory is provided, the default singularity cache is in the output directory.\n\n\n\npipeline.cache.image_cache(sub_args, config)\nAdds Docker Image URIs, or SIF paths to config if singularity cache option is provided. If singularity cache option is provided and a local SIF does not exist, a warning is displayed and the image will be pulled from URI in ‘config/containers/images.json’. @param sub_args &lt;parser.parse_args() object&gt;: Parsed arguments for run sub-command @params config : Docker Image config file @return config : Updated config dictionary containing user information (username and home directory)",
    "crumbs": [
      "Reference",
      "Modules",
      "pipeline.cache"
    ]
  },
  {
    "objectID": "reference/pipeline.cache.html#functions",
    "href": "reference/pipeline.cache.html#functions",
    "title": "pipeline.cache",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncheck_cache\nCheck if provided SINGULARITY_CACHE is valid. Singularity caches cannot be\n\n\nget_singularity_cachedir\nReturns the singularity cache directory.\n\n\nimage_cache\nAdds Docker Image URIs, or SIF paths to config if singularity cache option is provided.\n\n\n\n\n\npipeline.cache.check_cache(parser, cache, *args, **kwargs)\nCheck if provided SINGULARITY_CACHE is valid. Singularity caches cannot be shared across users (and must be owned by the user). Singularity strictly enforces 0700 user permission on on the cache directory and will return a non-zero exitcode. @param parser &lt;argparse.ArgumentParser() object&gt;: Argparse parser object @param cache : Singularity cache directory @return cache : If singularity cache dir is valid\n\n\n\npipeline.cache.get_singularity_cachedir(output_dir=None, cache_dir=None)\nReturns the singularity cache directory. If no user-provided cache directory is provided, the default singularity cache is in the output directory.\n\n\n\npipeline.cache.image_cache(sub_args, config)\nAdds Docker Image URIs, or SIF paths to config if singularity cache option is provided. If singularity cache option is provided and a local SIF does not exist, a warning is displayed and the image will be pulled from URI in ‘config/containers/images.json’. @param sub_args &lt;parser.parse_args() object&gt;: Parsed arguments for run sub-command @params config : Docker Image config file @return config : Updated config dictionary containing user information (username and home directory)",
    "crumbs": [
      "Reference",
      "Modules",
      "pipeline.cache"
    ]
  },
  {
    "objectID": "reference/jobby.html",
    "href": "reference/jobby.html",
    "title": "jobby",
    "section": "",
    "text": "jobby\nDisplay job information for past slurm job IDs\n\n\njobby will take your past jobs and display their job information. Why? We have pipelines running on several different clusters and job schedulers. jobby is an attempt to centralize and abstract the process of querying different job schedulers. On each supported target system, jobby will attempt to determine the best method for getting job information to return to the user in a standardized format and unified cli.\nMany thanks to the original author: Skyler Kuhn (@skchronicles)\nOriginal source: OpenOmics/mr-seek\n\n\n\n\npython&gt;=3.5\n\n\n\n\nPUBLIC DOMAIN NOTICE\n        NIAID Collaborative Bioinformatics Resource (NCBR)\n\n   National Institute of Allergy and Infectious Diseases (NIAID)\nThis software/database is a \"United  States Government Work\" under\nthe terms of the United  States Copyright Act.  It was written as\npart of the author's official duties as a United States Government\nemployee and thus cannot be copyrighted. This software is freely\navailable to the public for use.\n\nAlthough all  reasonable  efforts have been taken  to ensure  the\naccuracy and reliability of the software and data, NCBR do not and\ncannot warrant the performance or results that may  be obtained by\nusing this software or data. NCBR and NIH disclaim all warranties,\nexpress  or  implied,  including   warranties   of   performance,\nmerchantability or fitness for any particular purpose.\n\nPlease cite the author and NIH resources like the \"Biowulf Cluster\"\nin any work or product based on this material.\n\n\n\n$ jobby [OPTIONS] JOB_ID [JOB_ID …]\n\n\n\n$ jobby 18627545 15627516 58627597\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nColors\nClass encoding for ANSI escape sequences for styling terminal text.\n\n\n\n\n\njobby.Colors()\nClass encoding for ANSI escape sequences for styling terminal text. Any string that is formatting with these styles must be terminated with the escape sequence, i.e. Colors.end.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_missing\nAdds missing information to a list. This can be used\n\n\nconvert_size\nConverts bytes to a human readable format.\n\n\ndashboard_cli\nBiowulf-specific tool to get SLURM job information.\n\n\nerr\nPrints any provided args to standard error.\n\n\nfatal\nPrints any provided args to standard error\n\n\nget_toolkit\nFinds the best suited tool from a list of\n\n\njobby\nWrapper to each supported job scheduler: slurm, etc.\n\n\nparsed_arguments\nParses user-provided command-line arguments. This requires\n\n\nsacct\nGeneric tool to get SLURM job information.\n\n\nsge\nDisplays SGE job information to standard output.\n\n\nslurm\nDisplays SLURM job information to standard output.\n\n\nto_bytes\nConvert a human readable size unit into bytes.\n\n\nuge\nDisplays UGE job information to standard output.\n\n\nwhich\nChecks if an executable is in $PATH\n\n\n\n\n\njobby.add_missing(linelist, insertion_dict)\nAdds missing information to a list. This can be used to add missing job information fields to the results of job querying tool. @param linelist list[]: List containing job information for each field of interest @param insertion_dict dict[] = str Dictionary used to insert missing information to a given index, where the keys are indices of the linelist and the values are information to add. Please note that the indices should be zero based. Note that multiple consecutive values should be inserted at once as a list, see example below: Example: add_missing([0,1,2,3,4], {3:[‘+’,‘++’], 1:‘-’, 4:‘@’}) &gt;&gt; [0, ‘-’, 1, 2, ‘+’, ‘++’, 3, ‘@’, 4]\n\n\n\njobby.convert_size(size_bytes)\nConverts bytes to a human readable format.\n\n\n\njobby.dashboard_cli(jobs, threads=1, tmp_dir=None)\nBiowulf-specific tool to get SLURM job information. HPC staff recommend using this over the default slurm sacct command for performance reasons. By default, the dashboard_cli returns information for the following fields: jobid state submit_time partition nodes cpus mem timelimit gres dependency queued_time state_reason start_time elapsed_time end_time cpu_max mem_max eval Runs command: $ dashboard_cli jobs\n–joblist 12345679,12345680\n–fields FIELD,FIELD,FIELD\n–tab –archive\n\n\n\njobby.err(*message, **kwargs)\nPrints any provided args to standard error. kwargs can be provided to modify print functions behavior. @param message : Values printed to standard error @params kwargs &lt;print()&gt; Key words to modify print function behavior\n\n\n\njobby.fatal(*message, **kwargs)\nPrints any provided args to standard error and exits with an exit code of 1. @param message : Values printed to standard error @params kwargs &lt;print()&gt; Key words to modify print function behavior\n\n\n\njobby.get_toolkit(tool_list)\nFinds the best suited tool from a list of possible choices. Assumes tool list is already ordered from the best to worst choice. The first tool found in a user’s $PATH is returned. @param tool_list list[]: List of ordered tools to find @returns best_choice : First tool found in tool_list\n\n\n\njobby.jobby(args)\nWrapper to each supported job scheduler: slurm, etc. Each scheduler has a custom handler to most effectively get and parse job information. @param sub_args &lt;parser.parse_args() object&gt;: Parsed command-line arguments @return None\n\n\n\njobby.parsed_arguments(name, description)\nParses user-provided command-line arguments. This requires argparse and textwrap packages. To create custom help formatting a text wrapped docstring is used to create the help message for required options. As so, the help message for require options must be suppressed. If a new required argument is added to the cli, it must be updated in the usage statement docstring below. @param name : Name of the pipeline or command-line tool @param description : Short description of pipeline or command-line tool\n\n\n\njobby.sacct(jobs, threads=1, tmp_dir=None)\nGeneric tool to get SLURM job information. sacct should be available on all SLURM clusters. The dashboard_cli is prioritized over using sacct due to perform reasons; however, this method will be portable across different SLURM clusters. To get maximum memory usage for a job, we will need to parse the MaxRSS field from the $SLURM_JOBID.batch lines. Returns job information for the following fields: jobid jobname state partition reqtres alloccpus reqmem maxrss timelimit reserved start end elapsed nodelist user workdir To get maximum memory usage for a job, we will need to parse the MaxRSS fields from the $SLURM_JOBID.batch lines. Runs command: $ sacct -j 12345679,12345680\n–fields FIELD,FIELD,FIELD\n-P –delimiter $’ ’\n\n\n\njobby.sge(jobs, threads, tmp_dir)\nDisplays SGE job information to standard output. @param sub_args &lt;parser.parse_args() object&gt;: Parsed command-line arguments @return None\n\n\n\njobby.slurm(jobs, threads, tmp_dir)\nDisplays SLURM job information to standard output. @param sub_args &lt;parser.parse_args() object&gt;: Parsed command-line arguments @return None\n\n\n\njobby.to_bytes(size)\nConvert a human readable size unit into bytes. Returns None if cannot convert/parse provided size.\n\n\n\njobby.uge(jobs, threads, tmp_dir)\nDisplays UGE job information to standard output. @param sub_args &lt;parser.parse_args() object&gt;: Parsed command-line arguments @return None\n\n\n\njobby.which(cmd, path=None)\nChecks if an executable is in $PATH @param cmd : Name of executable to check @param path : Optional list of PATHs to check [default: $PATH] @return : True if exe in PATH, False if not in PATH",
    "crumbs": [
      "Reference",
      "Modules",
      "jobby"
    ]
  },
  {
    "objectID": "reference/jobby.html#about",
    "href": "reference/jobby.html#about",
    "title": "jobby",
    "section": "",
    "text": "jobby will take your past jobs and display their job information. Why? We have pipelines running on several different clusters and job schedulers. jobby is an attempt to centralize and abstract the process of querying different job schedulers. On each supported target system, jobby will attempt to determine the best method for getting job information to return to the user in a standardized format and unified cli.\nMany thanks to the original author: Skyler Kuhn (@skchronicles)\nOriginal source: OpenOmics/mr-seek",
    "crumbs": [
      "Reference",
      "Modules",
      "jobby"
    ]
  },
  {
    "objectID": "reference/jobby.html#requires",
    "href": "reference/jobby.html#requires",
    "title": "jobby",
    "section": "",
    "text": "python&gt;=3.5",
    "crumbs": [
      "Reference",
      "Modules",
      "jobby"
    ]
  },
  {
    "objectID": "reference/jobby.html#disclaimer",
    "href": "reference/jobby.html#disclaimer",
    "title": "jobby",
    "section": "",
    "text": "PUBLIC DOMAIN NOTICE\n        NIAID Collaborative Bioinformatics Resource (NCBR)\n\n   National Institute of Allergy and Infectious Diseases (NIAID)\nThis software/database is a \"United  States Government Work\" under\nthe terms of the United  States Copyright Act.  It was written as\npart of the author's official duties as a United States Government\nemployee and thus cannot be copyrighted. This software is freely\navailable to the public for use.\n\nAlthough all  reasonable  efforts have been taken  to ensure  the\naccuracy and reliability of the software and data, NCBR do not and\ncannot warrant the performance or results that may  be obtained by\nusing this software or data. NCBR and NIH disclaim all warranties,\nexpress  or  implied,  including   warranties   of   performance,\nmerchantability or fitness for any particular purpose.\n\nPlease cite the author and NIH resources like the \"Biowulf Cluster\"\nin any work or product based on this material.",
    "crumbs": [
      "Reference",
      "Modules",
      "jobby"
    ]
  },
  {
    "objectID": "reference/jobby.html#usage",
    "href": "reference/jobby.html#usage",
    "title": "jobby",
    "section": "",
    "text": "$ jobby [OPTIONS] JOB_ID [JOB_ID …]",
    "crumbs": [
      "Reference",
      "Modules",
      "jobby"
    ]
  },
  {
    "objectID": "reference/jobby.html#example",
    "href": "reference/jobby.html#example",
    "title": "jobby",
    "section": "",
    "text": "$ jobby 18627545 15627516 58627597",
    "crumbs": [
      "Reference",
      "Modules",
      "jobby"
    ]
  },
  {
    "objectID": "reference/jobby.html#classes",
    "href": "reference/jobby.html#classes",
    "title": "jobby",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nColors\nClass encoding for ANSI escape sequences for styling terminal text.\n\n\n\n\n\njobby.Colors()\nClass encoding for ANSI escape sequences for styling terminal text. Any string that is formatting with these styles must be terminated with the escape sequence, i.e. Colors.end.",
    "crumbs": [
      "Reference",
      "Modules",
      "jobby"
    ]
  },
  {
    "objectID": "reference/jobby.html#functions",
    "href": "reference/jobby.html#functions",
    "title": "jobby",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_missing\nAdds missing information to a list. This can be used\n\n\nconvert_size\nConverts bytes to a human readable format.\n\n\ndashboard_cli\nBiowulf-specific tool to get SLURM job information.\n\n\nerr\nPrints any provided args to standard error.\n\n\nfatal\nPrints any provided args to standard error\n\n\nget_toolkit\nFinds the best suited tool from a list of\n\n\njobby\nWrapper to each supported job scheduler: slurm, etc.\n\n\nparsed_arguments\nParses user-provided command-line arguments. This requires\n\n\nsacct\nGeneric tool to get SLURM job information.\n\n\nsge\nDisplays SGE job information to standard output.\n\n\nslurm\nDisplays SLURM job information to standard output.\n\n\nto_bytes\nConvert a human readable size unit into bytes.\n\n\nuge\nDisplays UGE job information to standard output.\n\n\nwhich\nChecks if an executable is in $PATH\n\n\n\n\n\njobby.add_missing(linelist, insertion_dict)\nAdds missing information to a list. This can be used to add missing job information fields to the results of job querying tool. @param linelist list[]: List containing job information for each field of interest @param insertion_dict dict[] = str Dictionary used to insert missing information to a given index, where the keys are indices of the linelist and the values are information to add. Please note that the indices should be zero based. Note that multiple consecutive values should be inserted at once as a list, see example below: Example: add_missing([0,1,2,3,4], {3:[‘+’,‘++’], 1:‘-’, 4:‘@’}) &gt;&gt; [0, ‘-’, 1, 2, ‘+’, ‘++’, 3, ‘@’, 4]\n\n\n\njobby.convert_size(size_bytes)\nConverts bytes to a human readable format.\n\n\n\njobby.dashboard_cli(jobs, threads=1, tmp_dir=None)\nBiowulf-specific tool to get SLURM job information. HPC staff recommend using this over the default slurm sacct command for performance reasons. By default, the dashboard_cli returns information for the following fields: jobid state submit_time partition nodes cpus mem timelimit gres dependency queued_time state_reason start_time elapsed_time end_time cpu_max mem_max eval Runs command: $ dashboard_cli jobs\n–joblist 12345679,12345680\n–fields FIELD,FIELD,FIELD\n–tab –archive\n\n\n\njobby.err(*message, **kwargs)\nPrints any provided args to standard error. kwargs can be provided to modify print functions behavior. @param message : Values printed to standard error @params kwargs &lt;print()&gt; Key words to modify print function behavior\n\n\n\njobby.fatal(*message, **kwargs)\nPrints any provided args to standard error and exits with an exit code of 1. @param message : Values printed to standard error @params kwargs &lt;print()&gt; Key words to modify print function behavior\n\n\n\njobby.get_toolkit(tool_list)\nFinds the best suited tool from a list of possible choices. Assumes tool list is already ordered from the best to worst choice. The first tool found in a user’s $PATH is returned. @param tool_list list[]: List of ordered tools to find @returns best_choice : First tool found in tool_list\n\n\n\njobby.jobby(args)\nWrapper to each supported job scheduler: slurm, etc. Each scheduler has a custom handler to most effectively get and parse job information. @param sub_args &lt;parser.parse_args() object&gt;: Parsed command-line arguments @return None\n\n\n\njobby.parsed_arguments(name, description)\nParses user-provided command-line arguments. This requires argparse and textwrap packages. To create custom help formatting a text wrapped docstring is used to create the help message for required options. As so, the help message for require options must be suppressed. If a new required argument is added to the cli, it must be updated in the usage statement docstring below. @param name : Name of the pipeline or command-line tool @param description : Short description of pipeline or command-line tool\n\n\n\njobby.sacct(jobs, threads=1, tmp_dir=None)\nGeneric tool to get SLURM job information. sacct should be available on all SLURM clusters. The dashboard_cli is prioritized over using sacct due to perform reasons; however, this method will be portable across different SLURM clusters. To get maximum memory usage for a job, we will need to parse the MaxRSS field from the $SLURM_JOBID.batch lines. Returns job information for the following fields: jobid jobname state partition reqtres alloccpus reqmem maxrss timelimit reserved start end elapsed nodelist user workdir To get maximum memory usage for a job, we will need to parse the MaxRSS fields from the $SLURM_JOBID.batch lines. Runs command: $ sacct -j 12345679,12345680\n–fields FIELD,FIELD,FIELD\n-P –delimiter $’ ’\n\n\n\njobby.sge(jobs, threads, tmp_dir)\nDisplays SGE job information to standard output. @param sub_args &lt;parser.parse_args() object&gt;: Parsed command-line arguments @return None\n\n\n\njobby.slurm(jobs, threads, tmp_dir)\nDisplays SLURM job information to standard output. @param sub_args &lt;parser.parse_args() object&gt;: Parsed command-line arguments @return None\n\n\n\njobby.to_bytes(size)\nConvert a human readable size unit into bytes. Returns None if cannot convert/parse provided size.\n\n\n\njobby.uge(jobs, threads, tmp_dir)\nDisplays UGE job information to standard output. @param sub_args &lt;parser.parse_args() object&gt;: Parsed command-line arguments @return None\n\n\n\njobby.which(cmd, path=None)\nChecks if an executable is in $PATH @param cmd : Name of executable to check @param path : Optional list of PATHs to check [default: $PATH] @return : True if exe in PATH, False if not in PATH",
    "crumbs": [
      "Reference",
      "Modules",
      "jobby"
    ]
  },
  {
    "objectID": "reference/shell.html",
    "href": "reference/shell.html",
    "title": "shell",
    "section": "",
    "text": "shell\nUtility functions for shell command execution.\n\n\n\n\n\nshell_run(“echo Hello, World!”) ’Hello, World!\n\n\n\n’ &gt;&gt;&gt; shell_run(“invalid_command”) ‘/bin/sh: invalid_command: command not found’\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconcat_newline\nConcatenates strings with a newline character between non-empty arguments\n\n\nexec_in_context\nExecutes a function in a context manager and captures the output from stdout and stderr.\n\n\nshell_run\nRun a shell command and return stdout/stderr\n\n\n\n\n\nshell.concat_newline(*args)\nConcatenates strings with a newline character between non-empty arguments\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\nstr\nVariable length argument list of strings to be concatenated.\n()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nstring\nstr\nThe concatenated string with newline characters between each non-empty argument.\n\n\n\n\n\n\n\nshell.exec_in_context(func, *args, **kwargs)\nExecutes a function in a context manager and captures the output from stdout and stderr.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nfunc\nThe function to be executed.\nrequired\n\n\n*args\nstr\nVariable length argument list to be passed to the function.\n()\n\n\n**kwargs\nstr\nArbitrary keyword arguments to be passed to the function.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nstr\nThe combined output from both stdout and stderr.\n\n\n\n\n\n\n\nshell.shell_run(\n    command_str\n    capture_output=True\n    check=True\n    shell=True\n    text=True\n)\nRun a shell command and return stdout/stderr\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncommand_str\nstr\nThe shell command to be executed.\nrequired\n\n\ncapture_output\nbool\nWhether to capture the command’s output. Defaults to True.\nTrue\n\n\ncheck\nbool\nWhether to raise an exception if the command returns a non-zero exit status. Defaults to True.\nTrue\n\n\nshell\nbool\nWhether to run the command through the shell. Defaults to True.\nTrue\n\n\ntext\nbool\nWhether to treat the command’s input/output as text. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nstr\nThe combined stdout and stderr of the command, separated by a newline character.",
    "crumbs": [
      "Reference",
      "Modules",
      "shell"
    ]
  },
  {
    "objectID": "reference/shell.html#example",
    "href": "reference/shell.html#example",
    "title": "shell",
    "section": "",
    "text": "shell_run(“echo Hello, World!”) ’Hello, World!\n\n\n\n’ &gt;&gt;&gt; shell_run(“invalid_command”) ‘/bin/sh: invalid_command: command not found’",
    "crumbs": [
      "Reference",
      "Modules",
      "shell"
    ]
  },
  {
    "objectID": "reference/shell.html#functions",
    "href": "reference/shell.html#functions",
    "title": "shell",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nconcat_newline\nConcatenates strings with a newline character between non-empty arguments\n\n\nexec_in_context\nExecutes a function in a context manager and captures the output from stdout and stderr.\n\n\nshell_run\nRun a shell command and return stdout/stderr\n\n\n\n\n\nshell.concat_newline(*args)\nConcatenates strings with a newline character between non-empty arguments\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\nstr\nVariable length argument list of strings to be concatenated.\n()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nstring\nstr\nThe concatenated string with newline characters between each non-empty argument.\n\n\n\n\n\n\n\nshell.exec_in_context(func, *args, **kwargs)\nExecutes a function in a context manager and captures the output from stdout and stderr.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nfunc\nThe function to be executed.\nrequired\n\n\n*args\nstr\nVariable length argument list to be passed to the function.\n()\n\n\n**kwargs\nstr\nArbitrary keyword arguments to be passed to the function.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nstr\nThe combined output from both stdout and stderr.\n\n\n\n\n\n\n\nshell.shell_run(\n    command_str\n    capture_output=True\n    check=True\n    shell=True\n    text=True\n)\nRun a shell command and return stdout/stderr\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncommand_str\nstr\nThe shell command to be executed.\nrequired\n\n\ncapture_output\nbool\nWhether to capture the command’s output. Defaults to True.\nTrue\n\n\ncheck\nbool\nWhether to raise an exception if the command returns a non-zero exit status. Defaults to True.\nTrue\n\n\nshell\nbool\nWhether to run the command through the shell. Defaults to True.\nTrue\n\n\ntext\nbool\nWhether to treat the command’s input/output as text. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nstr\nThe combined stdout and stderr of the command, separated by a newline character.",
    "crumbs": [
      "Reference",
      "Modules",
      "shell"
    ]
  },
  {
    "objectID": "reference/pkg_util.html",
    "href": "reference/pkg_util.html",
    "title": "pkg_util",
    "section": "",
    "text": "pkg_util\nMiscellaneous utility functions for the package\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_external_scripts\nGet list of standalone scripts included in the package\n\n\nget_package_version\nGet the current version of a package from the metadata.\n\n\nget_project_scripts\nGet a list of CLI tools in the package.\n\n\nget_pyproject_toml\nGet the contents of the package’s pyproject.toml file.\n\n\nget_version\nGet the current version of the ccbr_tools package.\n\n\nmsg\nPrints the error message with a timestamp.\n\n\nmsg_box\nDisplays a message box with a given splash message.\n\n\nprint_citation\nPrints the citation for the given citation file in the specified output format.\n\n\nrepo_base\nGet the absolute path to a file in the repository\n\n\n\n\n\npkg_util.get_external_scripts(pkg_name='ccbr_tools')\nGet list of standalone scripts included in the package\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npkg_name\nstr\nThe name of the package. Defaults to “ccbr_tools”.\n'ccbr_tools'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscripts\nlist\nA list of standalone scripts included in the package.\n\n\n\n\n\n\n\npkg_util.get_package_version(pkg_name='ccbr_tools')\nGet the current version of a package from the metadata.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npkg_name\nstr\nName of the package (default: ccbr_tools).\n'ccbr_tools'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nversion\nstr\nThe version of the package.\n\n\n\n\n\n\n\npkg_util.get_project_scripts(pkg_name='ccbr_tools')\nGet a list of CLI tools in the package.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npkg_name\nstr\nThe name of the package. Defaults to “ccbr_tools”.\n'ccbr_tools'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntools\nlist\nA sorted list of CLI tool names.\n\n\n\n\n\n\n\npkg_util.get_pyproject_toml(pkg_name='ccbr_tools', repo_base=repo_base)\nGet the contents of the package’s pyproject.toml file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npkg_name\nstr\nName of the package (default: ccbr_tools).\n'ccbr_tools'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npyproject\ndict\nThe contents of the pyproject.toml file.\n\n\n\n\n\n\n\npkg_util.get_version(repo_base=repo_base, debug=False)\nGet the current version of the ccbr_tools package.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo_base\nfunction\nA function that returns the base path of the repository.\nrepo_base\n\n\ndebug\nbool\nPrint the path to the VERSION file (default: False).\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nversion\nstr\nThe version of the package.\n\n\n\n\n\n\n\npkg_util.msg(err_message)\nPrints the error message with a timestamp.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nerr_message\nstr\nThe error message to be printed.\nrequired\n\n\n\nReturns: None\n\n\n\n\npkg_util.msg_box(splash, errmsg=None)\nDisplays a message box with a given splash message.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsplash\nstr\nThe splash message to be displayed.\nrequired\n\n\nerrmsg\nstr\nAn error message to be displayed below the splash message. Defaults to None.\nNone\n\n\n\n\n\n\n\npkg_util.print_citation(\n    citation_file=repo_base('CITATION.cff')\n    output_format='bibtex'\n)\nPrints the citation for the given citation file in the specified output format.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncitation_file\nstr\nThe path to the citation file.\nrepo_base('CITATION.cff')\n\n\noutput_format\nstr\nThe desired output format for the citation.\n'bibtex'\n\n\n\n\n\n\n\npkg_util.repo_base(*paths)\nGet the absolute path to a file in the repository\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*paths\nstr\nAdditional paths to join with the base path.\n()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nThe absolute path to the file in the repository.",
    "crumbs": [
      "Reference",
      "Modules",
      "pkg_util"
    ]
  },
  {
    "objectID": "reference/pkg_util.html#functions",
    "href": "reference/pkg_util.html#functions",
    "title": "pkg_util",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_external_scripts\nGet list of standalone scripts included in the package\n\n\nget_package_version\nGet the current version of a package from the metadata.\n\n\nget_project_scripts\nGet a list of CLI tools in the package.\n\n\nget_pyproject_toml\nGet the contents of the package’s pyproject.toml file.\n\n\nget_version\nGet the current version of the ccbr_tools package.\n\n\nmsg\nPrints the error message with a timestamp.\n\n\nmsg_box\nDisplays a message box with a given splash message.\n\n\nprint_citation\nPrints the citation for the given citation file in the specified output format.\n\n\nrepo_base\nGet the absolute path to a file in the repository\n\n\n\n\n\npkg_util.get_external_scripts(pkg_name='ccbr_tools')\nGet list of standalone scripts included in the package\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npkg_name\nstr\nThe name of the package. Defaults to “ccbr_tools”.\n'ccbr_tools'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscripts\nlist\nA list of standalone scripts included in the package.\n\n\n\n\n\n\n\npkg_util.get_package_version(pkg_name='ccbr_tools')\nGet the current version of a package from the metadata.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npkg_name\nstr\nName of the package (default: ccbr_tools).\n'ccbr_tools'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nversion\nstr\nThe version of the package.\n\n\n\n\n\n\n\npkg_util.get_project_scripts(pkg_name='ccbr_tools')\nGet a list of CLI tools in the package.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npkg_name\nstr\nThe name of the package. Defaults to “ccbr_tools”.\n'ccbr_tools'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntools\nlist\nA sorted list of CLI tool names.\n\n\n\n\n\n\n\npkg_util.get_pyproject_toml(pkg_name='ccbr_tools', repo_base=repo_base)\nGet the contents of the package’s pyproject.toml file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npkg_name\nstr\nName of the package (default: ccbr_tools).\n'ccbr_tools'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npyproject\ndict\nThe contents of the pyproject.toml file.\n\n\n\n\n\n\n\npkg_util.get_version(repo_base=repo_base, debug=False)\nGet the current version of the ccbr_tools package.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo_base\nfunction\nA function that returns the base path of the repository.\nrepo_base\n\n\ndebug\nbool\nPrint the path to the VERSION file (default: False).\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nversion\nstr\nThe version of the package.\n\n\n\n\n\n\n\npkg_util.msg(err_message)\nPrints the error message with a timestamp.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nerr_message\nstr\nThe error message to be printed.\nrequired\n\n\n\nReturns: None\n\n\n\n\npkg_util.msg_box(splash, errmsg=None)\nDisplays a message box with a given splash message.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsplash\nstr\nThe splash message to be displayed.\nrequired\n\n\nerrmsg\nstr\nAn error message to be displayed below the splash message. Defaults to None.\nNone\n\n\n\n\n\n\n\npkg_util.print_citation(\n    citation_file=repo_base('CITATION.cff')\n    output_format='bibtex'\n)\nPrints the citation for the given citation file in the specified output format.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncitation_file\nstr\nThe path to the citation file.\nrepo_base('CITATION.cff')\n\n\noutput_format\nstr\nThe desired output format for the citation.\n'bibtex'\n\n\n\n\n\n\n\npkg_util.repo_base(*paths)\nGet the absolute path to a file in the repository\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*paths\nstr\nAdditional paths to join with the base path.\n()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nThe absolute path to the file in the repository.",
    "crumbs": [
      "Reference",
      "Modules",
      "pkg_util"
    ]
  },
  {
    "objectID": "reference/intersect.html",
    "href": "reference/intersect.html",
    "title": "intersect",
    "section": "",
    "text": "intersect\nFind the intersect of two files, returns the inner join\nOriginal author: Skyler Kuhn (@skchronicles)\n\n\nintersect file1 file2",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "intersect"
    ]
  },
  {
    "objectID": "reference/intersect.html#usage",
    "href": "reference/intersect.html#usage",
    "title": "intersect",
    "section": "",
    "text": "intersect file1 file2",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "intersect"
    ]
  },
  {
    "objectID": "reference/pipeline.util.html",
    "href": "reference/pipeline.util.html",
    "title": "pipeline.util",
    "section": "",
    "text": "pipeline.util\nPipeline utility functions\n\n\n\n\n\nName\nDescription\n\n\n\n\nchmod_bins_exec\nEnsure that all files in bin/ are executable.\n\n\nerr\nPrints any provided args to standard error.\n\n\nexists\nChecks if file exists on the local filesystem.\n\n\nfatal\nPrints any provided args to standard error\n\n\nget_genomes_dict\nGet dictionary of genome annotation versions and the paths to the corresponding JSON files.\n\n\nget_genomes_list\nGet list of genome annotations available for the current platform\n\n\nget_hpcname\nGet the HPC name using scontrol\n\n\nget_tmp_dir\nGet default temporary directory for biowulf and frce. Allow user override.\n\n\ngit_commit_hash\nGets the git commit hash of the RNA-seek repo.\n\n\njoin_jsons\nJoins multiple JSON files to into one data structure\n\n\nln\nCreates symlinks for files to an output directory.\n\n\nmd5sum\nGets md5checksum of a file in memory-safe manner.\n\n\npermissions\nChecks permissions using os.access() to see the user is authorized to access\n\n\nrename\nDynamically renames FastQ file to have one of the following extensions: .R1.fastq.gz, .R2.fastq.gz\n\n\nrequire\nEnforces an executable is in $PATH\n\n\nsafe_copy\nPrivate function: Given a list paths it will recursively copy each to the\n\n\nscontrol_show\nRun scontrol show config and parse the output as a dictionary\n\n\nstandard_input\nChecks for standard input when provided or permissions using permissions().\n\n\nwhich\nChecks if an executable is in $PATH\n\n\n\n\n\npipeline.util.chmod_bins_exec(repo_base=repo_base)\nEnsure that all files in bin/ are executable.\nIt appears that setuptools strips executable permissions from package_data files, yet post-install scripts are not possible with the pyproject.toml format. Without this hack, nextflow processes that call scripts in bin/ fail.\nhttps://stackoverflow.com/questions/18409296/package-data-files-with-executable-permissions https://github.com/pypa/setuptools/issues/2041 https://stackoverflow.com/questions/76320274/post-install-script-for-pyproject-toml-projects\n\n\n\npipeline.util.err(*message, **kwargs)\nPrints any provided args to standard error. kwargs can be provided to modify print functions behavior. @param message : Values printed to standard error @params kwargs &lt;print()&gt; Key words to modify print function behavior\n\n\n\npipeline.util.exists(testpath)\nChecks if file exists on the local filesystem. @param parser &lt;argparse.ArgumentParser() object&gt;: argparse parser object @param testpath : Name of file/directory to check @return does_exist : True when file/directory exists, False when file/directory does not exist\n\n\n\npipeline.util.fatal(*message, **kwargs)\nPrints any provided args to standard error and exits with an exit code of 1. @param message : Values printed to standard error @params kwargs &lt;print()&gt; Key words to modify print function behavior\n\n\n\npipeline.util.get_genomes_dict(\n    repo_base\n    hpcname=get_hpcname()\n    error_on_warnings=False\n)\nGet dictionary of genome annotation versions and the paths to the corresponding JSON files.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo_base\nfunction\nFunction for getting the base directory of the repository.\nrequired\n\n\nhpcname\nstr\nName of the HPC. Defaults to the value returned by get_hpcname().\nget_hpcname()\n\n\nerror_on_warnings\nbool\nFlag to indicate whether to raise warnings as errors. Defaults to False.\nFalse\n\n\n\nReturns: genomes_dict (dict): A dictionary containing genome names as keys and corresponding JSON file paths as values. { genome_name: json_file_path }\n\n\n\n\npipeline.util.get_genomes_list(\n    repo_base\n    hpcname=get_hpcname()\n    error_on_warnings=False\n)\nGet list of genome annotations available for the current platform\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo_base\nstr\nThe base directory of the repository\nrequired\n\n\nhpcname\nstr\nThe name of the HPC. Defaults to the value returned by get_hpcname().\nget_hpcname()\n\n\nerror_on_warnings\nbool\nWhether to raise an error on warnings. Defaults to False.\nFalse\n\n\n\nReturns: genomes (list): A sorted list of genome annotations available for the current platform\n\n\n\n\npipeline.util.get_hpcname()\nGet the HPC name using scontrol\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nhpcname\nstr\nThe HPC name (biowulf, frce, or an empty string)\n\n\n\n\n\n\n\npipeline.util.get_tmp_dir(tmp_dir, outdir, hpc=get_hpcname())\nGet default temporary directory for biowulf and frce. Allow user override.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntmp_dir\nstr\nUser-defined temporary directory path. If provided, this path will be used as the temporary directory.\nrequired\n\n\noutdir\nstr\nOutput directory path.\nrequired\n\n\nhpc\nstr\nHPC name. Defaults to the value returned by get_hpcname().\nget_hpcname()\n\n\n\nReturns: tmp_dir (str): The default temporary directory path based on the HPC name and user-defined path.\n\n\n\n\npipeline.util.git_commit_hash(repo_path)\nGets the git commit hash of the RNA-seek repo. @param repo_path : Path to RNA-seek git repo @return githash : Latest git commit hash\n\n\n\npipeline.util.join_jsons(templates)\nJoins multiple JSON files to into one data structure Used to join multiple template JSON files to create a global config dictionary. @params templates &lt;list[str]&gt;: List of template JSON files to join together @return aggregated : Dictionary containing the contents of all the input JSON files\n\n\n\npipeline.util.ln(files, outdir)\nCreates symlinks for files to an output directory. @param files list[]: List of filenames @param outdir : Destination or output directory to create symlinks\n\n\n\npipeline.util.md5sum(filename, first_block_only=False, blocksize=65536)\nGets md5checksum of a file in memory-safe manner. The file is read in blocks/chunks defined by the blocksize parameter. This is a safer option to reading the entire file into memory if the file is very large. @param filename : Input file on local filesystem to find md5 checksum @param first_block_only : Calculate md5 checksum of the first block/chunk only @param blocksize : Blocksize of reading N chunks of data to reduce memory profile @return hasher.hexdigest() : MD5 checksum of the file’s contents\n\n\n\npipeline.util.permissions(parser, path, *args, **kwargs)\nChecks permissions using os.access() to see the user is authorized to access a file/directory. Checks for existence, readability, writability and executability via: os.F_OK (tests existence), os.R_OK (tests read), os.W_OK (tests write), os.X_OK (tests exec). @param parser &lt;argparse.ArgumentParser() object&gt;: Argparse parser object @param path : Name of path to check @return path : Returns abs path if it exists and permissions are correct\n\n\n\npipeline.util.rename(filename)\nDynamically renames FastQ file to have one of the following extensions: .R1.fastq.gz, .R2.fastq.gz To automatically rename the fastq files, a few assumptions are made. If the extension of the FastQ file cannot be inferred, an exception is raised telling the user to fix the filename of the fastq files. @param filename : Original name of file to be renamed @return filename : A renamed FastQ filename\n\n\n\npipeline.util.require(cmds, suggestions, path=None)\nEnforces an executable is in $PATH @param cmds list[]: List of executable names to check @param suggestions list[]: Name of module to suggest loading for a given index in param cmd. @param path list[]]: Optional list of PATHs to check [default: $PATH]\n\n\n\npipeline.util.safe_copy(source, target, resources=[])\nPrivate function: Given a list paths it will recursively copy each to the target location. If a target path already exists, it will NOT over-write the existing paths data. @param resources &lt;list[str]&gt;: List of paths to copy over to target location @params source : Add a prefix PATH to each resource @param target : Target path to copy templates and required resources\n\n\n\npipeline.util.scontrol_show()\nRun scontrol show config and parse the output as a dictionary\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscontrol_dict\ndict\ndictionary containing the output of scontrol show config\n\n\n\n\n\n\n\npipeline.util.standard_input(parser, path, *args, **kwargs)\nChecks for standard input when provided or permissions using permissions(). @param parser &lt;argparse.ArgumentParser() object&gt;: Argparse parser object @param path : Name of path to check @return path : If path exists and user can read from location\n\n\n\npipeline.util.which(cmd, path=None)\nChecks if an executable is in $PATH @param cmd : Name of executable to check @param path : Optional list of PATHs to check [default: $PATH] @return : True if exe in PATH, False if not in PATH",
    "crumbs": [
      "Reference",
      "Modules",
      "pipeline.util"
    ]
  },
  {
    "objectID": "reference/pipeline.util.html#functions",
    "href": "reference/pipeline.util.html#functions",
    "title": "pipeline.util",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nchmod_bins_exec\nEnsure that all files in bin/ are executable.\n\n\nerr\nPrints any provided args to standard error.\n\n\nexists\nChecks if file exists on the local filesystem.\n\n\nfatal\nPrints any provided args to standard error\n\n\nget_genomes_dict\nGet dictionary of genome annotation versions and the paths to the corresponding JSON files.\n\n\nget_genomes_list\nGet list of genome annotations available for the current platform\n\n\nget_hpcname\nGet the HPC name using scontrol\n\n\nget_tmp_dir\nGet default temporary directory for biowulf and frce. Allow user override.\n\n\ngit_commit_hash\nGets the git commit hash of the RNA-seek repo.\n\n\njoin_jsons\nJoins multiple JSON files to into one data structure\n\n\nln\nCreates symlinks for files to an output directory.\n\n\nmd5sum\nGets md5checksum of a file in memory-safe manner.\n\n\npermissions\nChecks permissions using os.access() to see the user is authorized to access\n\n\nrename\nDynamically renames FastQ file to have one of the following extensions: .R1.fastq.gz, .R2.fastq.gz\n\n\nrequire\nEnforces an executable is in $PATH\n\n\nsafe_copy\nPrivate function: Given a list paths it will recursively copy each to the\n\n\nscontrol_show\nRun scontrol show config and parse the output as a dictionary\n\n\nstandard_input\nChecks for standard input when provided or permissions using permissions().\n\n\nwhich\nChecks if an executable is in $PATH\n\n\n\n\n\npipeline.util.chmod_bins_exec(repo_base=repo_base)\nEnsure that all files in bin/ are executable.\nIt appears that setuptools strips executable permissions from package_data files, yet post-install scripts are not possible with the pyproject.toml format. Without this hack, nextflow processes that call scripts in bin/ fail.\nhttps://stackoverflow.com/questions/18409296/package-data-files-with-executable-permissions https://github.com/pypa/setuptools/issues/2041 https://stackoverflow.com/questions/76320274/post-install-script-for-pyproject-toml-projects\n\n\n\npipeline.util.err(*message, **kwargs)\nPrints any provided args to standard error. kwargs can be provided to modify print functions behavior. @param message : Values printed to standard error @params kwargs &lt;print()&gt; Key words to modify print function behavior\n\n\n\npipeline.util.exists(testpath)\nChecks if file exists on the local filesystem. @param parser &lt;argparse.ArgumentParser() object&gt;: argparse parser object @param testpath : Name of file/directory to check @return does_exist : True when file/directory exists, False when file/directory does not exist\n\n\n\npipeline.util.fatal(*message, **kwargs)\nPrints any provided args to standard error and exits with an exit code of 1. @param message : Values printed to standard error @params kwargs &lt;print()&gt; Key words to modify print function behavior\n\n\n\npipeline.util.get_genomes_dict(\n    repo_base\n    hpcname=get_hpcname()\n    error_on_warnings=False\n)\nGet dictionary of genome annotation versions and the paths to the corresponding JSON files.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo_base\nfunction\nFunction for getting the base directory of the repository.\nrequired\n\n\nhpcname\nstr\nName of the HPC. Defaults to the value returned by get_hpcname().\nget_hpcname()\n\n\nerror_on_warnings\nbool\nFlag to indicate whether to raise warnings as errors. Defaults to False.\nFalse\n\n\n\nReturns: genomes_dict (dict): A dictionary containing genome names as keys and corresponding JSON file paths as values. { genome_name: json_file_path }\n\n\n\n\npipeline.util.get_genomes_list(\n    repo_base\n    hpcname=get_hpcname()\n    error_on_warnings=False\n)\nGet list of genome annotations available for the current platform\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrepo_base\nstr\nThe base directory of the repository\nrequired\n\n\nhpcname\nstr\nThe name of the HPC. Defaults to the value returned by get_hpcname().\nget_hpcname()\n\n\nerror_on_warnings\nbool\nWhether to raise an error on warnings. Defaults to False.\nFalse\n\n\n\nReturns: genomes (list): A sorted list of genome annotations available for the current platform\n\n\n\n\npipeline.util.get_hpcname()\nGet the HPC name using scontrol\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nhpcname\nstr\nThe HPC name (biowulf, frce, or an empty string)\n\n\n\n\n\n\n\npipeline.util.get_tmp_dir(tmp_dir, outdir, hpc=get_hpcname())\nGet default temporary directory for biowulf and frce. Allow user override.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntmp_dir\nstr\nUser-defined temporary directory path. If provided, this path will be used as the temporary directory.\nrequired\n\n\noutdir\nstr\nOutput directory path.\nrequired\n\n\nhpc\nstr\nHPC name. Defaults to the value returned by get_hpcname().\nget_hpcname()\n\n\n\nReturns: tmp_dir (str): The default temporary directory path based on the HPC name and user-defined path.\n\n\n\n\npipeline.util.git_commit_hash(repo_path)\nGets the git commit hash of the RNA-seek repo. @param repo_path : Path to RNA-seek git repo @return githash : Latest git commit hash\n\n\n\npipeline.util.join_jsons(templates)\nJoins multiple JSON files to into one data structure Used to join multiple template JSON files to create a global config dictionary. @params templates &lt;list[str]&gt;: List of template JSON files to join together @return aggregated : Dictionary containing the contents of all the input JSON files\n\n\n\npipeline.util.ln(files, outdir)\nCreates symlinks for files to an output directory. @param files list[]: List of filenames @param outdir : Destination or output directory to create symlinks\n\n\n\npipeline.util.md5sum(filename, first_block_only=False, blocksize=65536)\nGets md5checksum of a file in memory-safe manner. The file is read in blocks/chunks defined by the blocksize parameter. This is a safer option to reading the entire file into memory if the file is very large. @param filename : Input file on local filesystem to find md5 checksum @param first_block_only : Calculate md5 checksum of the first block/chunk only @param blocksize : Blocksize of reading N chunks of data to reduce memory profile @return hasher.hexdigest() : MD5 checksum of the file’s contents\n\n\n\npipeline.util.permissions(parser, path, *args, **kwargs)\nChecks permissions using os.access() to see the user is authorized to access a file/directory. Checks for existence, readability, writability and executability via: os.F_OK (tests existence), os.R_OK (tests read), os.W_OK (tests write), os.X_OK (tests exec). @param parser &lt;argparse.ArgumentParser() object&gt;: Argparse parser object @param path : Name of path to check @return path : Returns abs path if it exists and permissions are correct\n\n\n\npipeline.util.rename(filename)\nDynamically renames FastQ file to have one of the following extensions: .R1.fastq.gz, .R2.fastq.gz To automatically rename the fastq files, a few assumptions are made. If the extension of the FastQ file cannot be inferred, an exception is raised telling the user to fix the filename of the fastq files. @param filename : Original name of file to be renamed @return filename : A renamed FastQ filename\n\n\n\npipeline.util.require(cmds, suggestions, path=None)\nEnforces an executable is in $PATH @param cmds list[]: List of executable names to check @param suggestions list[]: Name of module to suggest loading for a given index in param cmd. @param path list[]]: Optional list of PATHs to check [default: $PATH]\n\n\n\npipeline.util.safe_copy(source, target, resources=[])\nPrivate function: Given a list paths it will recursively copy each to the target location. If a target path already exists, it will NOT over-write the existing paths data. @param resources &lt;list[str]&gt;: List of paths to copy over to target location @params source : Add a prefix PATH to each resource @param target : Target path to copy templates and required resources\n\n\n\npipeline.util.scontrol_show()\nRun scontrol show config and parse the output as a dictionary\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscontrol_dict\ndict\ndictionary containing the output of scontrol show config\n\n\n\n\n\n\n\npipeline.util.standard_input(parser, path, *args, **kwargs)\nChecks for standard input when provided or permissions using permissions(). @param parser &lt;argparse.ArgumentParser() object&gt;: Argparse parser object @param path : Name of path to check @return path : If path exists and user can read from location\n\n\n\npipeline.util.which(cmd, path=None)\nChecks if an executable is in $PATH @param cmd : Name of executable to check @param path : Optional list of PATHs to check [default: $PATH] @return : True if exe in PATH, False if not in PATH",
    "crumbs": [
      "Reference",
      "Modules",
      "pipeline.util"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to CCBR Tools",
    "section": "",
    "text": "If you want to make a change, it’s a good idea to first open an issue and make sure someone from the team agrees that it’s needed.\nIf you’ve decided to work on an issue, assign yourself to the issue so others will know you’re working on it.\n\n\n\nWe use GitHub Flow as our collaboration process. Follow the steps below for detailed instructions on contributing changes to CCBR Tools.\n\n\n\nGitHub Flow diagram\n\n\n\n\nIf you are a member of CCBR, you can clone this repository to your computer or development environment. Otherwise, you will first need to fork the repo and clone your fork. You only need to do this step once.\ngit clone https://github.com/CCBR/tools\n\nCloning into ‘tools’…  remote: Enumerating objects: 1136, done.  remote: Counting objects: 100% (463/463), done.  remote: Compressing objects: 100% (357/357), done.  remote: Total 1136 (delta 149), reused 332 (delta 103), pack-reused 673  Receiving objects: 100% (1136/1136), 11.01 MiB | 9.76 MiB/s, done.  Resolving deltas: 100% (530/530), done. \n\ncd tools\n\n\n\n\nInstall the python dependencies with pip\npip install .[[dev,test]]\nInstall pre-commit if you don’t already have it. Then from the repo’s root directory, run\npre-commit install\nThis will install the repo’s pre-commit hooks. You’ll only need to do this step the first time you clone the repo.\n\n\n\n\nCreate a Git branch for your pull request (PR). Give the branch a descriptive name for the changes you will make, such as iss-10 if it is for a specific issue.\n# create a new branch and switch to it\ngit branch iss-10\ngit switch iss-10\n\nSwitched to a new branch ‘iss-10’\n\n\n\n\nEdit the code, write and run tests, and update the documentation as needed.\n\n\nChanges to the python package code will also need unit tests to demonstrate that the changes work as intended. We write unit tests with pytest and store them in the tests/ subdirectory. Run the tests with python -m pytest.\n\n\n\nIf you have added a new feature or changed the API of an existing feature, you will likely need to update the documentation in docs/. If the changes are in src/, you may need to update the docstrings – we use Google Style for Python code.\n\n\n\n\nIf you’re not sure how often you should commit or what your commits should consist of, we recommend following the “atomic commits” principle where each commit contains one new feature, fix, or task. Learn more about atomic commits here: https://www.freshconsulting.com/insights/blog/atomic-commits/\nFirst, add the files that you changed to the staging area:\ngit add path/to/changed/files/\nThen make the commit. Your commit message should follow the Conventional Commits specification. Briefly, each commit should start with one of the approved types such as feat, fix, docs, etc. followed by a description of the commit. Take a look at the Conventional Commits specification for more detailed information about how to write commit messages.\ngit commit -m 'feat: create function for awesome feature'\npre-commit will enforce that your commit message and the code changes are styled correctly and will attempt to make corrections if needed.\n\nCheck for added large files……………………………………….Passed  Fix End of Files…………………………………………………Passed  Trim Trailing Whitespace………………………………………….Failed \n\nhook id: trailing-whitespace \nexit code: 1 \nfiles were modified by this hook  &gt;  Fixing path/to/changed/files/file.txt  &gt;  codespell……………………………………………………….Passed  style-files……………………………………(no files to check)Skipped  readme-rmd-rendered…………………………….(no files to check)Skipped  use-tidy-description……………………………(no files to check)Skipped \n\n\nIn the example above, one of the hooks modified a file in the proposed commit, so the pre-commit check failed. You can run git diff to see the changes that pre-commit made and git status to see which files were modified. To proceed with the commit, re-add the modified file(s) and re-run the commit command:\ngit add path/to/changed/files/file.txt\ngit commit -m 'feat: create function for awesome feature'\nThis time, all the hooks either passed or were skipped (e.g. hooks that only run on R code will not run if no R files were committed). When the pre-commit check is successful, the usual commit success message will appear after the pre-commit messages showing that the commit was created.\n\nCheck for added large files……………………………………….Passed  Fix End of Files…………………………………………………Passed  Trim Trailing Whitespace………………………………………….Passed  codespell……………………………………………………….Passed  style-files……………………………………(no files to check)Skipped  readme-rmd-rendered…………………………….(no files to check)Skipped  use-tidy-description……………………………(no files to check)Skipped  Conventional Commit………………………………………………Passed  &gt; [iss-10 9ff256e] feat: create function for awesome feature  1 file changed, 22 insertions(+), 3 deletions(-) \n\nFinally, push your changes to GitHub:\ngit push\nIf this is the first time you are pushing this branch, you may have to explicitly set the upstream branch:\ngit push --set-upstream origin iss-10\n\nEnumerating objects: 7, done.  Counting objects: 100% (7/7), done.  Delta compression using up to 10 threads  Compressing objects: 100% (4/4), done.  Writing objects: 100% (4/4), 648 bytes | 648.00 KiB/s, done.  Total 4 (delta 3), reused 0 (delta 0), pack-reused 0  remote: Resolving deltas: 100% (3/3), completed with 3 local objects.  remote:  remote: Create a pull request for ‘iss-10’ on GitHub by visiting:  remote: https://github.com/CCBR/tools/pull/new/iss-10  remote:  To https://github.com/CCBR/tools  &gt;  &gt; [new branch] iss-10 -&gt; iss-10  branch ‘iss-10’ set up to track ‘origin/iss-10’. \n\nWe recommend pushing your commits often so they will be backed up on GitHub. You can view the files in your branch on GitHub at https://github.com/CCBR/tools/tree/&lt;your-branch-name&gt; (replace &lt;your-branch-name&gt; with the actual name of your branch).\n\n\n\nOnce your branch is ready, create a PR on GitHub: https://github.com/CCBR/tools/pull/new/\nSelect the branch you just pushed:\n\n\n\nCreate a new PR from your branch\n\n\nEdit the PR title and description. The title should briefly describe the change. Follow the comments in the template to fill out the body of the PR, and you can delete the comments (everything between &lt;!-- and --&gt;) as you go. Be sure to fill out the checklist, checking off items as you complete them or striking through any irrelevant items. When you’re ready, click ‘Create pull request’ to open it.\n\n\n\nOpen the PR after editing the title and description\n\n\nOptionally, you can mark the PR as a draft if you’re not yet ready for it to be reviewed, then change it later when you’re ready.\n\n\n\nWe will do our best to follow the tidyverse code review principles: https://code-review.tidyverse.org/. The reviewer may suggest that you make changes before accepting your PR in order to improve the code quality or style. If that’s the case, continue to make changes in your branch and push them to GitHub, and they will appear in the PR.\nOnce the PR is approved, the maintainer will merge it and the issue(s) the PR links will close automatically. Congratulations and thank you for your contribution!\n\n\n\nAfter your PR has been merged, update your local clone of the repo by switching to the main branch and pulling the latest changes:\ngit checkout main\ngit pull\nIt’s a good idea to run git pull before creating a new branch so it will start from the most recent commits in main.\n\n\n\n\n\nGitHub Flow\nsemantic versioning guidelines\nchangelog guidelines\ntidyverse code review principles\nreproducible examples"
  },
  {
    "objectID": "CONTRIBUTING.html#proposing-changes-with-issues",
    "href": "CONTRIBUTING.html#proposing-changes-with-issues",
    "title": "Contributing to CCBR Tools",
    "section": "",
    "text": "If you want to make a change, it’s a good idea to first open an issue and make sure someone from the team agrees that it’s needed.\nIf you’ve decided to work on an issue, assign yourself to the issue so others will know you’re working on it."
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-process",
    "href": "CONTRIBUTING.html#pull-request-process",
    "title": "Contributing to CCBR Tools",
    "section": "",
    "text": "We use GitHub Flow as our collaboration process. Follow the steps below for detailed instructions on contributing changes to CCBR Tools.\n\n\n\nGitHub Flow diagram\n\n\n\n\nIf you are a member of CCBR, you can clone this repository to your computer or development environment. Otherwise, you will first need to fork the repo and clone your fork. You only need to do this step once.\ngit clone https://github.com/CCBR/tools\n\nCloning into ‘tools’…  remote: Enumerating objects: 1136, done.  remote: Counting objects: 100% (463/463), done.  remote: Compressing objects: 100% (357/357), done.  remote: Total 1136 (delta 149), reused 332 (delta 103), pack-reused 673  Receiving objects: 100% (1136/1136), 11.01 MiB | 9.76 MiB/s, done.  Resolving deltas: 100% (530/530), done. \n\ncd tools\n\n\n\n\nInstall the python dependencies with pip\npip install .[[dev,test]]\nInstall pre-commit if you don’t already have it. Then from the repo’s root directory, run\npre-commit install\nThis will install the repo’s pre-commit hooks. You’ll only need to do this step the first time you clone the repo.\n\n\n\n\nCreate a Git branch for your pull request (PR). Give the branch a descriptive name for the changes you will make, such as iss-10 if it is for a specific issue.\n# create a new branch and switch to it\ngit branch iss-10\ngit switch iss-10\n\nSwitched to a new branch ‘iss-10’\n\n\n\n\nEdit the code, write and run tests, and update the documentation as needed.\n\n\nChanges to the python package code will also need unit tests to demonstrate that the changes work as intended. We write unit tests with pytest and store them in the tests/ subdirectory. Run the tests with python -m pytest.\n\n\n\nIf you have added a new feature or changed the API of an existing feature, you will likely need to update the documentation in docs/. If the changes are in src/, you may need to update the docstrings – we use Google Style for Python code.\n\n\n\n\nIf you’re not sure how often you should commit or what your commits should consist of, we recommend following the “atomic commits” principle where each commit contains one new feature, fix, or task. Learn more about atomic commits here: https://www.freshconsulting.com/insights/blog/atomic-commits/\nFirst, add the files that you changed to the staging area:\ngit add path/to/changed/files/\nThen make the commit. Your commit message should follow the Conventional Commits specification. Briefly, each commit should start with one of the approved types such as feat, fix, docs, etc. followed by a description of the commit. Take a look at the Conventional Commits specification for more detailed information about how to write commit messages.\ngit commit -m 'feat: create function for awesome feature'\npre-commit will enforce that your commit message and the code changes are styled correctly and will attempt to make corrections if needed.\n\nCheck for added large files……………………………………….Passed  Fix End of Files…………………………………………………Passed  Trim Trailing Whitespace………………………………………….Failed \n\nhook id: trailing-whitespace \nexit code: 1 \nfiles were modified by this hook  &gt;  Fixing path/to/changed/files/file.txt  &gt;  codespell……………………………………………………….Passed  style-files……………………………………(no files to check)Skipped  readme-rmd-rendered…………………………….(no files to check)Skipped  use-tidy-description……………………………(no files to check)Skipped \n\n\nIn the example above, one of the hooks modified a file in the proposed commit, so the pre-commit check failed. You can run git diff to see the changes that pre-commit made and git status to see which files were modified. To proceed with the commit, re-add the modified file(s) and re-run the commit command:\ngit add path/to/changed/files/file.txt\ngit commit -m 'feat: create function for awesome feature'\nThis time, all the hooks either passed or were skipped (e.g. hooks that only run on R code will not run if no R files were committed). When the pre-commit check is successful, the usual commit success message will appear after the pre-commit messages showing that the commit was created.\n\nCheck for added large files……………………………………….Passed  Fix End of Files…………………………………………………Passed  Trim Trailing Whitespace………………………………………….Passed  codespell……………………………………………………….Passed  style-files……………………………………(no files to check)Skipped  readme-rmd-rendered…………………………….(no files to check)Skipped  use-tidy-description……………………………(no files to check)Skipped  Conventional Commit………………………………………………Passed  &gt; [iss-10 9ff256e] feat: create function for awesome feature  1 file changed, 22 insertions(+), 3 deletions(-) \n\nFinally, push your changes to GitHub:\ngit push\nIf this is the first time you are pushing this branch, you may have to explicitly set the upstream branch:\ngit push --set-upstream origin iss-10\n\nEnumerating objects: 7, done.  Counting objects: 100% (7/7), done.  Delta compression using up to 10 threads  Compressing objects: 100% (4/4), done.  Writing objects: 100% (4/4), 648 bytes | 648.00 KiB/s, done.  Total 4 (delta 3), reused 0 (delta 0), pack-reused 0  remote: Resolving deltas: 100% (3/3), completed with 3 local objects.  remote:  remote: Create a pull request for ‘iss-10’ on GitHub by visiting:  remote: https://github.com/CCBR/tools/pull/new/iss-10  remote:  To https://github.com/CCBR/tools  &gt;  &gt; [new branch] iss-10 -&gt; iss-10  branch ‘iss-10’ set up to track ‘origin/iss-10’. \n\nWe recommend pushing your commits often so they will be backed up on GitHub. You can view the files in your branch on GitHub at https://github.com/CCBR/tools/tree/&lt;your-branch-name&gt; (replace &lt;your-branch-name&gt; with the actual name of your branch).\n\n\n\nOnce your branch is ready, create a PR on GitHub: https://github.com/CCBR/tools/pull/new/\nSelect the branch you just pushed:\n\n\n\nCreate a new PR from your branch\n\n\nEdit the PR title and description. The title should briefly describe the change. Follow the comments in the template to fill out the body of the PR, and you can delete the comments (everything between &lt;!-- and --&gt;) as you go. Be sure to fill out the checklist, checking off items as you complete them or striking through any irrelevant items. When you’re ready, click ‘Create pull request’ to open it.\n\n\n\nOpen the PR after editing the title and description\n\n\nOptionally, you can mark the PR as a draft if you’re not yet ready for it to be reviewed, then change it later when you’re ready.\n\n\n\nWe will do our best to follow the tidyverse code review principles: https://code-review.tidyverse.org/. The reviewer may suggest that you make changes before accepting your PR in order to improve the code quality or style. If that’s the case, continue to make changes in your branch and push them to GitHub, and they will appear in the PR.\nOnce the PR is approved, the maintainer will merge it and the issue(s) the PR links will close automatically. Congratulations and thank you for your contribution!\n\n\n\nAfter your PR has been merged, update your local clone of the repo by switching to the main branch and pulling the latest changes:\ngit checkout main\ngit pull\nIt’s a good idea to run git pull before creating a new branch so it will start from the most recent commits in main."
  },
  {
    "objectID": "CONTRIBUTING.html#helpful-links-for-more-information",
    "href": "CONTRIBUTING.html#helpful-links-for-more-information",
    "title": "Contributing to CCBR Tools",
    "section": "",
    "text": "GitHub Flow\nsemantic versioning guidelines\nchangelog guidelines\ntidyverse code review principles\nreproducible examples"
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "CCBR Tools",
    "section": "",
    "text": "use major & minor version for docs website subdirectories. (#15, @kelly-sovacool)"
  },
  {
    "objectID": "CHANGELOG.html#tools-development-version",
    "href": "CHANGELOG.html#tools-development-version",
    "title": "CCBR Tools",
    "section": "",
    "text": "use major & minor version for docs website subdirectories. (#15, @kelly-sovacool)"
  },
  {
    "objectID": "CHANGELOG.html#tools-0.1.1",
    "href": "CHANGELOG.html#tools-0.1.1",
    "title": "CCBR Tools",
    "section": "Tools 0.1.1",
    "text": "Tools 0.1.1\n\nfix: don’t add extra newline to command stdout/stderr for shell_run() and exec_in_context(). (#10, @kelly-sovacool)\nminor docuemntation improvements. (#12, @kelly-sovacool)"
  },
  {
    "objectID": "CHANGELOG.html#tools-0.1.0",
    "href": "CHANGELOG.html#tools-0.1.0",
    "title": "CCBR Tools",
    "section": "Tools 0.1.0",
    "text": "Tools 0.1.0\nThe Tools repository is now restructured as a Python package. All previous python scripts which included command line utilities have been moved to src/, and all other scripts have been moved to scripts/. In both cases, they are available in the path when the package is installed.\nFunctions which were part of both XAVIER and RENEE are available for re-use in other bioinformatics pipelines for tasks such as determining the HPC environment, retrieving available genome annotations, and printing citation and version information. Explore the ccbr_tools reference documentation for more information: https://ccbr.github.io/Tools/latest/reference/\n\nCLI Utilities\nCommand-line utilities in CCBR Tools.\n\nccbr_tools\ngb2gtf\nhf\nintersect\njobby\njobinfo\npeek\n\nRun a command with --help to learn how to use it.\n\n\nExternal Scripts\nAdditional standalone scripts for various common tasks in scripts/ are added to the path when this package is installed. They are less robust than the CLI Utilities included in the package and do not have any unit tests.\n\nadd_gene_name_to_count_matrix.R\naggregate_data_tables.R\nargparse.bash\ncancel_snakemake_jobs.sh\ncreate_hpc_link.sh\nextract_value_from_json.py\nextract_value_from_yaml.py\nfilter_bam_by_readids.py\nfilter_fastq_by_readids_highmem.py\nfilter_fastq_by_readids_highmem_pe.py\ngather_cluster_stats.sh\ngather_cluster_stats_biowulf.sh\nget_buyin_partition_list.bash\nget_slurm_file_with_error.sh\ngsea_preranked.sh\nkaryoploter.R\nmake_labels_for_pipeliner.sh\nrawcounts2normalizedcounts_DESeq2.R\nrawcounts2normalizedcounts_limmavoom.R\nrun_jobby_on_nextflow_log\nrun_jobby_on_nextflow_log_full_format\nrun_jobby_on_snakemake_log\nrun_jobby_on_snakemake_log_full_format\nspooker\nwhich_vpn.sh"
  },
  {
    "objectID": "CHANGELOG.html#tools-0.0.1",
    "href": "CHANGELOG.html#tools-0.0.1",
    "title": "CCBR Tools",
    "section": "Tools 0.0.1",
    "text": "Tools 0.0.1\nThis tag marks the repository state from before refactoring it as a python package."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CCBR Tools",
    "section": "",
    "text": "Utilities for CCBR Bioinformatics Software\n   \n\n\nOn biowulf you can access the latest release of ccbr_tools by loading the ccbrpipeliner module:\nmodule load ccbrpipeliner\nOutside of biowulf, you can install the package with pip:\npip install git+https://github.com/CCBR/Tools\nOr specify a specific tagged version or branch:\npip install git+https://github.com/CCBR/Tools@v0.1.0\n\n\n\n\n\nccbr_tools --help\nUsage: ccbr_tools [OPTIONS] COMMAND [ARGS]...\n\n  Utilities for CCBR Bioinformatics Software\n\n  For more options, run: tool_name [command] --help\n\nOptions:\n  -v, --version  Show the version and exit.\n  -h, --help     Show this message and exit.\n\nCommands:\n  cite     Print the citation in the desired format\n  version  Print the version of ccbr_tools\n\nAll installed tools:\n  ccbr_tools\n  gb2gtf\n  hf\n  intersect\n  jobby\n  jobinfo\n  peek\n\n\n\nimport ccbr_tools.pkg_util\nprint(ccbr_tools.pkg_util.get_version())\n0.1.1-dev\n\n\n\n\nCommand-line utilities in CCBR Tools.\n\nccbr_tools\ngb2gtf\nhf\nintersect\njobby\njobinfo\npeek\n\nRun a command with --help to learn how to use it.\n\n\n\nAdditional standalone scripts for various common tasks in scripts/ are added to the path when this package is installed. They are less robust than the CLI Utilities included in the package and do not have any unit tests.\n\nadd_gene_name_to_count_matrix.R\naggregate_data_tables.R\nargparse.bash\ncancel_snakemake_jobs.sh\ncreate_hpc_link.sh\nextract_value_from_json.py\nextract_value_from_yaml.py\nfilter_bam_by_readids.py\nfilter_fastq_by_readids_highmem.py\nfilter_fastq_by_readids_highmem_pe.py\ngather_cluster_stats.sh\ngather_cluster_stats_biowulf.sh\nget_buyin_partition_list.bash\nget_slurm_file_with_error.sh\ngsea_preranked.sh\nkaryoploter.R\nmake_labels_for_pipeliner.sh\nrawcounts2normalizedcounts_DESeq2.R\nrawcounts2normalizedcounts_limmavoom.R\nrun_jobby_on_nextflow_log\nrun_jobby_on_nextflow_log_full_format\nrun_jobby_on_snakemake_log\nrun_jobby_on_snakemake_log_full_format\nspooker\nwhich_vpn.sh\n\n\n\n\nPlease cite this software if you use it in a publication:\n\nSovacool K., Koparde V., Kuhn S., Tandon M., Huse S. (2024). CCBR Tools: Utilities for CCBR Bioinformatics Software (version v0.1.0). DOI: 10.5281/zenodo.13377166 URL: https://ccbr.github.io/Tools/\n\n\n\n@misc{YourReferenceHere,\nauthor = {Sovacool, Kelly and Koparde, Vishal and Kuhn, Skyler and Tandon, Mayank and Huse, Susan},\ndoi = {10.5281/zenodo.13377166},\nmonth = {8},\ntitle = {CCBR Tools: Utilities for CCBR Bioinformatics Software},\nurl = {https://ccbr.github.io/Tools/},\nyear = {2024}\n}"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "CCBR Tools",
    "section": "",
    "text": "On biowulf you can access the latest release of ccbr_tools by loading the ccbrpipeliner module:\nmodule load ccbrpipeliner\nOutside of biowulf, you can install the package with pip:\npip install git+https://github.com/CCBR/Tools\nOr specify a specific tagged version or branch:\npip install git+https://github.com/CCBR/Tools@v0.1.0"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "CCBR Tools",
    "section": "",
    "text": "ccbr_tools --help\nUsage: ccbr_tools [OPTIONS] COMMAND [ARGS]...\n\n  Utilities for CCBR Bioinformatics Software\n\n  For more options, run: tool_name [command] --help\n\nOptions:\n  -v, --version  Show the version and exit.\n  -h, --help     Show this message and exit.\n\nCommands:\n  cite     Print the citation in the desired format\n  version  Print the version of ccbr_tools\n\nAll installed tools:\n  ccbr_tools\n  gb2gtf\n  hf\n  intersect\n  jobby\n  jobinfo\n  peek\n\n\n\nimport ccbr_tools.pkg_util\nprint(ccbr_tools.pkg_util.get_version())\n0.1.1-dev"
  },
  {
    "objectID": "index.html#cli-utilities",
    "href": "index.html#cli-utilities",
    "title": "CCBR Tools",
    "section": "",
    "text": "Command-line utilities in CCBR Tools.\n\nccbr_tools\ngb2gtf\nhf\nintersect\njobby\njobinfo\npeek\n\nRun a command with --help to learn how to use it."
  },
  {
    "objectID": "index.html#external-scripts",
    "href": "index.html#external-scripts",
    "title": "CCBR Tools",
    "section": "",
    "text": "Additional standalone scripts for various common tasks in scripts/ are added to the path when this package is installed. They are less robust than the CLI Utilities included in the package and do not have any unit tests.\n\nadd_gene_name_to_count_matrix.R\naggregate_data_tables.R\nargparse.bash\ncancel_snakemake_jobs.sh\ncreate_hpc_link.sh\nextract_value_from_json.py\nextract_value_from_yaml.py\nfilter_bam_by_readids.py\nfilter_fastq_by_readids_highmem.py\nfilter_fastq_by_readids_highmem_pe.py\ngather_cluster_stats.sh\ngather_cluster_stats_biowulf.sh\nget_buyin_partition_list.bash\nget_slurm_file_with_error.sh\ngsea_preranked.sh\nkaryoploter.R\nmake_labels_for_pipeliner.sh\nrawcounts2normalizedcounts_DESeq2.R\nrawcounts2normalizedcounts_limmavoom.R\nrun_jobby_on_nextflow_log\nrun_jobby_on_nextflow_log_full_format\nrun_jobby_on_snakemake_log\nrun_jobby_on_snakemake_log_full_format\nspooker\nwhich_vpn.sh"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "CCBR Tools",
    "section": "",
    "text": "Please cite this software if you use it in a publication:\n\nSovacool K., Koparde V., Kuhn S., Tandon M., Huse S. (2024). CCBR Tools: Utilities for CCBR Bioinformatics Software (version v0.1.0). DOI: 10.5281/zenodo.13377166 URL: https://ccbr.github.io/Tools/\n\n\n\n@misc{YourReferenceHere,\nauthor = {Sovacool, Kelly and Koparde, Vishal and Kuhn, Skyler and Tandon, Mayank and Huse, Susan},\ndoi = {10.5281/zenodo.13377166},\nmonth = {8},\ntitle = {CCBR Tools: Utilities for CCBR Bioinformatics Software},\nurl = {https://ccbr.github.io/Tools/},\nyear = {2024}\n}"
  },
  {
    "objectID": "reference/gb2gtf.html",
    "href": "reference/gb2gtf.html",
    "title": "gb2gtf",
    "section": "",
    "text": "gb2gtf\nModule for converting GenBank files to GTF format.\n\n\npython gb2gtf.py sequence.gb &gt; sequence.gtf",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "gb2gtf"
    ]
  },
  {
    "objectID": "reference/gb2gtf.html#usage",
    "href": "reference/gb2gtf.html#usage",
    "title": "gb2gtf",
    "section": "",
    "text": "python gb2gtf.py sequence.gb &gt; sequence.gtf",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "gb2gtf"
    ]
  },
  {
    "objectID": "reference/GSEA.ncbr_huse.html",
    "href": "reference/GSEA.ncbr_huse.html",
    "title": "GSEA.ncbr_huse",
    "section": "",
    "text": "GSEA.ncbr_huse\nGSEA.ncbr_huse\nSet of functions supporting the FNL NCBR work\nAuthor: Susan Huse\nCreated on Mon Aug 6 11:07:30 2018",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "GSEA.ncbr_huse"
    ]
  },
  {
    "objectID": "reference/jobinfo.html",
    "href": "reference/jobinfo.html",
    "title": "jobinfo",
    "section": "",
    "text": "jobinfo\nGet HPC usage metadata for a list of slurm jobids on biowulf\n\n\nThis wrapper script works only on BIOWULF! This script usage the “dashboard_cli” utility on biowulf to get HPC usage metadata for a list of slurm jobids. These slurm jobids can be either provided at command line or extracted from a snakemake.log file. Using snakemake.log file option together with –failonly option lists path to the STDERR files for failed jobs. This can be very useful to debug failed Snakemake workflows.\n\n\n\n$ jobinfo -h\n\n\n\n$ jobinfo -j 123456,7891011 $ jobinfo -s /path/to/snakemake.log $ jobinfo -j 123456,7891011 -o /path/to/report.tsv $ jobinfo -s /path/to/snakemake.log –failonly\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_help\ncheck if usage needs to be printed\n\n\nexit_w_msg\nGracefully exit with proper message\n\n\n\n\n\njobinfo.check_help(parser)\ncheck if usage needs to be printed\n\n\n\njobinfo.exit_w_msg(message)\nGracefully exit with proper message",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "jobinfo"
    ]
  },
  {
    "objectID": "reference/jobinfo.html#about",
    "href": "reference/jobinfo.html#about",
    "title": "jobinfo",
    "section": "",
    "text": "This wrapper script works only on BIOWULF! This script usage the “dashboard_cli” utility on biowulf to get HPC usage metadata for a list of slurm jobids. These slurm jobids can be either provided at command line or extracted from a snakemake.log file. Using snakemake.log file option together with –failonly option lists path to the STDERR files for failed jobs. This can be very useful to debug failed Snakemake workflows.",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "jobinfo"
    ]
  },
  {
    "objectID": "reference/jobinfo.html#usage",
    "href": "reference/jobinfo.html#usage",
    "title": "jobinfo",
    "section": "",
    "text": "$ jobinfo -h",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "jobinfo"
    ]
  },
  {
    "objectID": "reference/jobinfo.html#example",
    "href": "reference/jobinfo.html#example",
    "title": "jobinfo",
    "section": "",
    "text": "$ jobinfo -j 123456,7891011 $ jobinfo -s /path/to/snakemake.log $ jobinfo -j 123456,7891011 -o /path/to/report.tsv $ jobinfo -s /path/to/snakemake.log –failonly",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "jobinfo"
    ]
  },
  {
    "objectID": "reference/jobinfo.html#functions",
    "href": "reference/jobinfo.html#functions",
    "title": "jobinfo",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncheck_help\ncheck if usage needs to be printed\n\n\nexit_w_msg\nGracefully exit with proper message\n\n\n\n\n\njobinfo.check_help(parser)\ncheck if usage needs to be printed\n\n\n\njobinfo.exit_w_msg(message)\nGracefully exit with proper message",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "jobinfo"
    ]
  },
  {
    "objectID": "reference/homologfinder.hf.html",
    "href": "reference/homologfinder.hf.html",
    "title": "homologfinder.hf",
    "section": "",
    "text": "homologfinder.hf\nFinds homologs in human and mouse.\n\n\nhf or HomologFinder finds homologs in human and mouse. if the input gene or genelist is human, then it returns mouse homolog(s) and vice versa\n\n\n\n$ hf -h\n\n\n\n$ hf -g ZNF365\n$ hf -l Wdr53,Zfp365\n$ hf -f genelist.txt\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_help\ncheck if usage needs to be printed\n\n\ncollect_args\ncollect all the cli arguments\n\n\nexit_w_msg\nGracefully exit with proper message\n\n\n\n\n\nhomologfinder.hf.check_help(parser)\ncheck if usage needs to be printed\n\n\n\nhomologfinder.hf.collect_args()\ncollect all the cli arguments\n\n\n\nhomologfinder.hf.exit_w_msg(message)\nGracefully exit with proper message",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "homologfinder.hf"
    ]
  },
  {
    "objectID": "reference/homologfinder.hf.html#about",
    "href": "reference/homologfinder.hf.html#about",
    "title": "homologfinder.hf",
    "section": "",
    "text": "hf or HomologFinder finds homologs in human and mouse. if the input gene or genelist is human, then it returns mouse homolog(s) and vice versa",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "homologfinder.hf"
    ]
  },
  {
    "objectID": "reference/homologfinder.hf.html#usage",
    "href": "reference/homologfinder.hf.html#usage",
    "title": "homologfinder.hf",
    "section": "",
    "text": "$ hf -h",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "homologfinder.hf"
    ]
  },
  {
    "objectID": "reference/homologfinder.hf.html#examples",
    "href": "reference/homologfinder.hf.html#examples",
    "title": "homologfinder.hf",
    "section": "",
    "text": "$ hf -g ZNF365\n$ hf -l Wdr53,Zfp365\n$ hf -f genelist.txt",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "homologfinder.hf"
    ]
  },
  {
    "objectID": "reference/homologfinder.hf.html#functions",
    "href": "reference/homologfinder.hf.html#functions",
    "title": "homologfinder.hf",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncheck_help\ncheck if usage needs to be printed\n\n\ncollect_args\ncollect all the cli arguments\n\n\nexit_w_msg\nGracefully exit with proper message\n\n\n\n\n\nhomologfinder.hf.check_help(parser)\ncheck if usage needs to be printed\n\n\n\nhomologfinder.hf.collect_args()\ncollect all the cli arguments\n\n\n\nhomologfinder.hf.exit_w_msg(message)\nGracefully exit with proper message",
    "crumbs": [
      "Reference",
      "Legacy tools",
      "homologfinder.hf"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "API Reference",
    "section": "",
    "text": "jobby\nDisplay job information for past slurm job IDs\n\n\npeek\nTake a peek at tab-delimited files\n\n\npipeline.cache\nFunctions for singularity cache management\n\n\npipeline.hpc\nClasses for working with different HPC clusters.\n\n\npipeline.nextflow\nRun Nextflow workflows in local and HPC environments.\n\n\npipeline.util\nPipeline utility functions\n\n\npkg_util\nMiscellaneous utility functions for the package\n\n\nshell\nUtility functions for shell command execution.\n\n\ntemplates\nTemplate files for CCBR Tools.\n\n\n\n\n\n\n\n\n\nGSEA.deg2gs\nReads a rnaseq pipeliner *_DEG_all_genes.txt file and outputs a prioritized list of Ensembl gene IDs for ToppFun\n\n\nGSEA.multitext2excel\nReads a list of files to import as separate tabs in Excel\n\n\nGSEA.ncbr_huse\nSet of functions supporting the FNL NCBR work\n\n\ngb2gtf\nModule for converting GenBank files to GTF format.\n\n\nhomologfinder.hf\nFinds homologs in human and mouse.\n\n\nintersect\nFind the intersect of two files, returns the inner join\n\n\njobinfo\nGet HPC usage metadata for a list of slurm jobids on biowulf",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#modules",
    "href": "reference/index.html#modules",
    "title": "API Reference",
    "section": "",
    "text": "jobby\nDisplay job information for past slurm job IDs\n\n\npeek\nTake a peek at tab-delimited files\n\n\npipeline.cache\nFunctions for singularity cache management\n\n\npipeline.hpc\nClasses for working with different HPC clusters.\n\n\npipeline.nextflow\nRun Nextflow workflows in local and HPC environments.\n\n\npipeline.util\nPipeline utility functions\n\n\npkg_util\nMiscellaneous utility functions for the package\n\n\nshell\nUtility functions for shell command execution.\n\n\ntemplates\nTemplate files for CCBR Tools.",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#legacy-tools",
    "href": "reference/index.html#legacy-tools",
    "title": "API Reference",
    "section": "",
    "text": "GSEA.deg2gs\nReads a rnaseq pipeliner *_DEG_all_genes.txt file and outputs a prioritized list of Ensembl gene IDs for ToppFun\n\n\nGSEA.multitext2excel\nReads a list of files to import as separate tabs in Excel\n\n\nGSEA.ncbr_huse\nSet of functions supporting the FNL NCBR work\n\n\ngb2gtf\nModule for converting GenBank files to GTF format.\n\n\nhomologfinder.hf\nFinds homologs in human and mouse.\n\n\nintersect\nFind the intersect of two files, returns the inner join\n\n\njobinfo\nGet HPC usage metadata for a list of slurm jobids on biowulf",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/templates.html",
    "href": "reference/templates.html",
    "title": "templates",
    "section": "",
    "text": "templates\nTemplate files for CCBR Tools.\n\n\n\nccbr_tools.templates.submit_slurm.sh\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nread_template\nRead a template file\n\n\nuse_template\nUses a template, formats variables, and writes it to a file.\n\n\n\n\n\ntemplates.read_template(template_name)\nRead a template file\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntemplate_name\nstr\nName of the template file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntemplate\nstr\nContents of the template file\n\n\n\n\n\n\n\ntemplates.use_template(template_name, output_filepath=None, **kwargs)\nUses a template, formats variables, and writes it to a file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntemplate_name\nstr\nThe name of the template to use.\nrequired\n\n\noutput_filepath\nstr\nThe filepath to save the output file. If not provided, it will be written to template_name in the current working directory.\nNone\n\n\n**kwargs\nstr\nKeyword arguments to fill in the template variables.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the template file is not found.\n\n\n\nIOError\nIf there is an error writing the output file.\n\n\n\n\n\n\nuse_template(“submit_slurm.sh”, output_filepath=“./submit_slurm.sh”, PIPELINE=“CCBR_nxf”, MODULES=“ccbrpipeliner nextflow”, ENV_VARS=““, RUN_COMMAND=”nextflow run main.nf -stub”)",
    "crumbs": [
      "Reference",
      "Modules",
      "templates"
    ]
  },
  {
    "objectID": "reference/templates.html#templates",
    "href": "reference/templates.html#templates",
    "title": "templates",
    "section": "",
    "text": "ccbr_tools.templates.submit_slurm.sh",
    "crumbs": [
      "Reference",
      "Modules",
      "templates"
    ]
  },
  {
    "objectID": "reference/templates.html#functions",
    "href": "reference/templates.html#functions",
    "title": "templates",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nread_template\nRead a template file\n\n\nuse_template\nUses a template, formats variables, and writes it to a file.\n\n\n\n\n\ntemplates.read_template(template_name)\nRead a template file\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntemplate_name\nstr\nName of the template file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntemplate\nstr\nContents of the template file\n\n\n\n\n\n\n\ntemplates.use_template(template_name, output_filepath=None, **kwargs)\nUses a template, formats variables, and writes it to a file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntemplate_name\nstr\nThe name of the template to use.\nrequired\n\n\noutput_filepath\nstr\nThe filepath to save the output file. If not provided, it will be written to template_name in the current working directory.\nNone\n\n\n**kwargs\nstr\nKeyword arguments to fill in the template variables.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the template file is not found.\n\n\n\nIOError\nIf there is an error writing the output file.\n\n\n\n\n\n\nuse_template(“submit_slurm.sh”, output_filepath=“./submit_slurm.sh”, PIPELINE=“CCBR_nxf”, MODULES=“ccbrpipeliner nextflow”, ENV_VARS=““, RUN_COMMAND=”nextflow run main.nf -stub”)",
    "crumbs": [
      "Reference",
      "Modules",
      "templates"
    ]
  },
  {
    "objectID": "reference/pipeline.hpc.html",
    "href": "reference/pipeline.hpc.html",
    "title": "pipeline.hpc",
    "section": "",
    "text": "pipeline.hpc\nClasses for working with different HPC clusters.\nUse ccbr_tools.pipeline.hpc.get_hpc to retrieve an HPC Cluster instance, which contains default attributes for supported clusters.\n\n\n\n\n\nName\nDescription\n\n\n\n\nBiowulf\nThe Biowulf cluster – child of ccbr_tools.pipeline.hpc.Cluster\n\n\nCluster\nBase class for an HPC cluster - evaluates to None\n\n\nFRCE\nThe FRCE cluster – child of ccbr_tools.pipeline.hpc.Cluster\n\n\n\n\n\npipeline.hpc.Biowulf(self)\nThe Biowulf cluster – child of ccbr_tools.pipeline.hpc.Cluster\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the cluster.\n\n\nmodules\ndict\nA dictionary mapping module names to their corresponding commands.\n\n\nsingularity_sif_dir\nstr\nThe directory path for Singularity SIF files.\n\n\nenv_vars\nstr\nA string representing the environment variables to be set on the cluster.\n\n\n\n\n\n\n\npipeline.hpc.Cluster(self)\nBase class for an HPC cluster - evaluates to None\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the cluster.\n\n\nmodules\ndict\nA dictionary containing the modules installed on the cluster. The keys are the module names and the values are the corresponding versions.\n\n\nsingularity_sif_dir\nstr\nThe directory where Singularity SIF files are stored.\n\n\nenv_vars\nstr\nA string representing the environment variables to be set on the cluster.\n\n\n\n\n\n\n\npipeline.hpc.FRCE(self)\nThe FRCE cluster – child of ccbr_tools.pipeline.hpc.Cluster\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the cluster.\n\n\nmodules\ndict\nA dictionary mapping module names to their corresponding commands.\n\n\nsingularity_sif_dir\nstr\nThe directory path for Singularity SIF files.\n\n\nenv_vars\nstr\nA string representing the environment variables to be set on the cluster.\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_hpc\nReturns an instance of the High-Performance Computing (HPC) cluster based on the specified HPC name.\n\n\n\n\n\npipeline.hpc.get_hpc(debug=False)\nReturns an instance of the High-Performance Computing (HPC) cluster based on the specified HPC name.\nIf the HPC is not known or supported, an instance of the base Cluster class is returned.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndebug\nbool\nIf True, uses debug as the HPC name. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncluster\nCluster\nAn instance of the HPC cluster.\n\n\n\n\n\n\n&gt;&gt;&gt; get_hpc()\n&gt;&gt;&gt; get_hpc(debug=True)",
    "crumbs": [
      "Reference",
      "Modules",
      "pipeline.hpc"
    ]
  },
  {
    "objectID": "reference/pipeline.hpc.html#classes",
    "href": "reference/pipeline.hpc.html#classes",
    "title": "pipeline.hpc",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nBiowulf\nThe Biowulf cluster – child of ccbr_tools.pipeline.hpc.Cluster\n\n\nCluster\nBase class for an HPC cluster - evaluates to None\n\n\nFRCE\nThe FRCE cluster – child of ccbr_tools.pipeline.hpc.Cluster\n\n\n\n\n\npipeline.hpc.Biowulf(self)\nThe Biowulf cluster – child of ccbr_tools.pipeline.hpc.Cluster\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the cluster.\n\n\nmodules\ndict\nA dictionary mapping module names to their corresponding commands.\n\n\nsingularity_sif_dir\nstr\nThe directory path for Singularity SIF files.\n\n\nenv_vars\nstr\nA string representing the environment variables to be set on the cluster.\n\n\n\n\n\n\n\npipeline.hpc.Cluster(self)\nBase class for an HPC cluster - evaluates to None\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the cluster.\n\n\nmodules\ndict\nA dictionary containing the modules installed on the cluster. The keys are the module names and the values are the corresponding versions.\n\n\nsingularity_sif_dir\nstr\nThe directory where Singularity SIF files are stored.\n\n\nenv_vars\nstr\nA string representing the environment variables to be set on the cluster.\n\n\n\n\n\n\n\npipeline.hpc.FRCE(self)\nThe FRCE cluster – child of ccbr_tools.pipeline.hpc.Cluster\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the cluster.\n\n\nmodules\ndict\nA dictionary mapping module names to their corresponding commands.\n\n\nsingularity_sif_dir\nstr\nThe directory path for Singularity SIF files.\n\n\nenv_vars\nstr\nA string representing the environment variables to be set on the cluster.",
    "crumbs": [
      "Reference",
      "Modules",
      "pipeline.hpc"
    ]
  },
  {
    "objectID": "reference/pipeline.hpc.html#functions",
    "href": "reference/pipeline.hpc.html#functions",
    "title": "pipeline.hpc",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_hpc\nReturns an instance of the High-Performance Computing (HPC) cluster based on the specified HPC name.\n\n\n\n\n\npipeline.hpc.get_hpc(debug=False)\nReturns an instance of the High-Performance Computing (HPC) cluster based on the specified HPC name.\nIf the HPC is not known or supported, an instance of the base Cluster class is returned.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndebug\nbool\nIf True, uses debug as the HPC name. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncluster\nCluster\nAn instance of the HPC cluster.\n\n\n\n\n\n\n&gt;&gt;&gt; get_hpc()\n&gt;&gt;&gt; get_hpc(debug=True)",
    "crumbs": [
      "Reference",
      "Modules",
      "pipeline.hpc"
    ]
  },
  {
    "objectID": "reference/peek.html",
    "href": "reference/peek.html",
    "title": "peek",
    "section": "",
    "text": "peek\nTake a peek at tab-delimited files\n\n\npeek &lt;file.tsv&gt; [buffer]\n\n\n\n\n\n\nName\nDescription\n\n\n\n\njustify\nCalculates the spacing for justifying to the right\n\n\nmax_string\nGiven a list of strings, finds the maximum strign length\n\n\npargs\nBasic command-line parser\n\n\npprint\nRe-formats first two lines on file so columns are left justified and values are right justified\n\n\nprint_header\nPrint filenames and divider\n\n\nusage\nPrint usage information and exit program\n\n\n\n\n\npeek.justify(h, d, n, nr)\nCalculates the spacing for justifying to the right\n\n\n\npeek.max_string(data)\nGiven a list of strings, finds the maximum strign length\n\n\n\npeek.pargs()\nBasic command-line parser\n\n\n\npeek.pprint(headlist, data, linelength, fn)\nRe-formats first two lines on file so columns are left justified and values are right justified\n\n\n\npeek.print_header(filename, length)\nPrint filenames and divider\n\n\n\npeek.usage()\nPrint usage information and exit program",
    "crumbs": [
      "Reference",
      "Modules",
      "peek"
    ]
  },
  {
    "objectID": "reference/peek.html#usage",
    "href": "reference/peek.html#usage",
    "title": "peek",
    "section": "",
    "text": "peek &lt;file.tsv&gt; [buffer]",
    "crumbs": [
      "Reference",
      "Modules",
      "peek"
    ]
  },
  {
    "objectID": "reference/peek.html#functions",
    "href": "reference/peek.html#functions",
    "title": "peek",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\njustify\nCalculates the spacing for justifying to the right\n\n\nmax_string\nGiven a list of strings, finds the maximum strign length\n\n\npargs\nBasic command-line parser\n\n\npprint\nRe-formats first two lines on file so columns are left justified and values are right justified\n\n\nprint_header\nPrint filenames and divider\n\n\nusage\nPrint usage information and exit program\n\n\n\n\n\npeek.justify(h, d, n, nr)\nCalculates the spacing for justifying to the right\n\n\n\npeek.max_string(data)\nGiven a list of strings, finds the maximum strign length\n\n\n\npeek.pargs()\nBasic command-line parser\n\n\n\npeek.pprint(headlist, data, linelength, fn)\nRe-formats first two lines on file so columns are left justified and values are right justified\n\n\n\npeek.print_header(filename, length)\nPrint filenames and divider\n\n\n\npeek.usage()\nPrint usage information and exit program",
    "crumbs": [
      "Reference",
      "Modules",
      "peek"
    ]
  }
]